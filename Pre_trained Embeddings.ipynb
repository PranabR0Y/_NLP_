{"cells":[{"cell_type":"markdown","metadata":{"id":"cFVXxbxzJX2a"},"source":["Pre-trained embeddings are word (or token) vector representations that have already been trained on massive text corpora by other researchers or organizations — so you can reuse them instead of training from scratch.\n","\n","⚙️ What They Contain\n","\n","Each word (or subword) is represented by a dense vector, typically of size 50–300 dimensions (or 768+ for transformer models).\n","\n","The embeddings capture:\n","\n","1. Semantic similarity → “king” and “queen” are close.\n","\n","2. Syntactic similarity → “walk”, “walking”, “walked” have similar vectors.\n","\n","3. Analogy relationships → “king - man + woman ≈ queen”.\n","\n","| Type                              | Trained On                      | When You Train                          |\n","| --------------------------------- | ------------------------------- | --------------------------------------- |\n","| **Custom Word2Vec**               | Your small dataset              | When you call `Word2Vec(sentences=...)` |\n","| **Pre-trained Word2Vec (Google)** | Google News (100 billion words) | Already done — just load and use        |\n","| **Pre-trained GloVe**             | Wikipedia + Gigaword            | Already done — just load                |\n","| **Pre-trained FastText**          | Common Crawl (600B tokens)      | Already done — just load                |\n","| **Pre-trained BERT**              | BookCorpus + Wikipedia          | Already done — just fine-tune           |\n","\n","#Popular Pre-trained Embedding Models\n","| Model                           | Creator                | Vector Size | Corpus                     | Key Feature                     |\n","| ------------------------------- | ---------------------- | ----------- | -------------------------- | ------------------------------- |\n","| **Word2Vec (Google News)**      | Google                 | 300D        | Google News (100B words)   | Classic skip-gram model         |\n","| **GloVe (6B / 42B / 840B)**     | Stanford               | 50D–300D    | Wikipedia + Common Crawl   | Global co-occurrence model      |\n","| **FastText**                    | Facebook               | 300D        | Common Crawl (600B tokens) | Uses subword info for OOV words |\n","| **ELMo**                        | Allen AI               | 1024D       | 1B Word Benchmark          | Contextual embeddings           |\n","| **BERT / RoBERTa / GPT / etc.** | Google / Meta / OpenAI | 768–1024D   | Massive web + books        | Deep contextual embeddings      |\n","\n","Embeddings (Word2Vec, GloVe, FastText) are static — each word has one fixed vector, regardless of context.\n","\n","Modern transformer-based models like BERT, GPT, RoBERTa, DistilBERT introduced contextual embeddings:\n"]},{"cell_type":"markdown","metadata":{"id":"nIORdZ-30ARt"},"source":["#ELMO(Embedding from Language Models)\n","A deep contextual word representation model. It was the first major leap beyond Word2Vec / GloVe —\n","because it generates different embeddings for the same word depending on its context.\n","\n","ELMo is based on a bidirectional language model (biLM) built with two LSTM layers.\n","\n","1. Input Layer (Character-based)\n","\n","    Each word is represented using character-level CNNs, not just whole-word tokens.\n","    \"playing\" → ['p', 'l', 'a', 'y', 'i', 'n', 'g']\n","\n","    ELMo uses character convolutions to build word embeddings.\n","2. Bidirectional Language Model (biLM)\n","\n","    ELMo trains two LSTMs:\n","\n","    One forward LSTM → predicts the next word given the previous context\n","\n","    One backward LSTM → predicts the previous word given the next context\n","\n"," 3. Concatenating Contextual States\n","\n","    For each word, ELMo takes:\n","\n","    The forward hidden state\n","\n","    The backward hidden state\n","\n","    Then concatenates them:\n","\n","   ` ELMo(word) = [h_forward; h_backward]`\n","\n","4. Layer Combination (Task-Specific)\n","\n","    ELMo doesn’t just use one LSTM layer — it uses multiple layers, each capturing different linguistic information.\n","\n","| Aspect         | ELMo                                   |\n","| -------------- | -------------------------------------- |\n","| Developer      | Allen Institute for AI (2018)          |\n","| Architecture   | Character CNN + 2-layer BiLSTM         |\n","| Embedding size | 1024                                   |\n","| Contextual     | ✅ Yes                                  |\n","| Handles OOV    | ✅ Yes                                  |\n","| Key innovation | Dynamic, context-dependent embeddings  |\n","| Replaced by    | BERT, RoBERTa, GPT (transformer-based) |\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"958asEIWJyIu"},"outputs":[],"source":["!pip install tensorflow tensorflow-hub tensorflow-text"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":9951,"status":"ok","timestamp":1761070150942,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"},"user_tz":-360},"id":"cJCWVj7wImAc"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n","import numpy as np\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","import pandas as pd\n","from sklearn.utils import resample\n","import tensorflow as tf\n","\n","elmo = hub.KerasLayer(\"https://tfhub.dev/google/elmo/3\", trainable=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":549,"status":"ok","timestamp":1761023620489,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"},"user_tz":-360},"id":"9g0zFd0UKBHk","outputId":"48f7020f-fa7a-4882-ab20-16b1a896df43"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2, 1024)\n","[-0.7354043   0.18077965 -0.24448149 ... -0.06355472  0.21377377\n"," -0.13779095]\n","[-0.26127908 -0.1274916  -0.23593295  0.07491662  0.0320407  -0.08880552\n"," -0.13739316  0.3712413   0.09359346  0.13650009]\n"]}],"source":["#Sentence Level Embeddings\n","\n","sentences = [\n","    \"The bank approved my loan.\",\n","    \"He sat on the river bank.\"\n","]\n","\n","# Get ELMo embeddings\n","embeddings = elmo(sentences)\n","\n","print(embeddings.shape)\n","import numpy as np\n","\n","# Convert TensorFlow tensor to NumPy array\n","embeddings_np = embeddings.numpy()\n","\n","# Print first sentence embedding\n","print(embeddings_np[0])\n","\n","\n","print(embeddings_np[1][:10])"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":3845,"status":"ok","timestamp":1761072078930,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"},"user_tz":-360},"id":"qkhF9umjLiBl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0d246135-88f9-41fb-9830-e77fa710d5fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Word embeddings shape: (2, 6, 1024)\n","(1024,)\n","(6, 1024)\n","Sentence 1: (6, 1024)\n","Sentence 2: (6, 1024)\n"]}],"source":["#Word Level Embeddings\n","\n","elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n","\n","# Access the \"default\" signature\n","elmo_sign = elmo.signatures[\"default\"]\n","\n","# Prepare your text input\n","sentences = [\n","    \"The bank approved my loan.\", #6  tokens (“The”, “bank”, “approved”, “my”, “loan”, “.”), each mapped to a 1024-dimensional contextual embedding.\n","    \"He sat on the river bank.\"\n","]\n","\n","# Get embeddings\n","embeddings = elmo_sign(tf.constant(sentences))\n","\n","# Get the word-level ELMo embeddings\n","word_emb = embeddings[\"elmo\"]\n","\n","print(\"Word embeddings shape:\", word_emb.shape)\n","#print(\"First word vector (first 5 values):\")\n","print(word_emb[0][0].numpy().shape)\n","print(word_emb[1].shape)\n","\n","for i, sent in enumerate(sentences):\n","    print(f\"Sentence {i+1}: {word_emb[i].shape}\")"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSa40SksLmBWlmAOPrF/O5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}