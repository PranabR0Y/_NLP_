{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPAa8/8e/BdnOYPU5JM54y3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Implementation from TensorFlow Hub"],"metadata":{"id":"zaVWMArsLZEe"}},{"cell_type":"code","source":["!pip install tensorflow_text"],"metadata":{"id":"blYniw1ra5lk","executionInfo":{"status":"ok","timestamp":1761155061528,"user_tz":-360,"elapsed":2,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"JD27D3J0ZqI4","executionInfo":{"status":"ok","timestamp":1761155071515,"user_tz":-360,"elapsed":8087,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}}},"outputs":[],"source":["import tensorflow_hub as hub\n","import tensorflow_text as text\n","from sklearn.utils import resample"]},{"cell_type":"code","source":["preprocessor = hub.KerasLayer( \"https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3\")\n","model = hub.KerasLayer(\"https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/bert-en-uncased-l-6-h-768-a-12/2\",trainable=True)\n"],"metadata":{"id":"l6f1MZbUaZu7","executionInfo":{"status":"ok","timestamp":1761155083555,"user_tz":-360,"elapsed":12018,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["#What the preprocessor does\n","\n","The preprocessor prepares raw text so that it can be fed into the BERT model.\n","\n","Think of it as a text-to-tensors converter.\n","It handles all the text preprocessing that’s required for BERT to understand the input.\n","\n","| Step                         | Description                                                                                                        |\n","| ---------------------------- | ------------------------------------------------------------------------------------------------------------------ |\n","| **Tokenization**             | Splits text into word pieces (subwords) using the *WordPiece* tokenizer. Example: “playing” → [“play”, “##ing”]    |\n","| **Lowercasing**              | Because it’s an “uncased” model, it converts text to lowercase.                                                    |\n","| **Adding special tokens**    | Adds `[CLS]` (classification token) at the start and `[SEP]` (separator token) between or at the end of sentences. |\n","| **Creating attention masks** | Marks which tokens are real (1) vs. padding (0) so the model ignores padding.                                      |\n","| **Creating segment IDs**     | If you have two sentences, marks which tokens belong to sentence A and which to sentence B.                        |\n","| **Output formatting**        | Produces a dictionary of tensors:                                                                                  |\n","\n","Output Format:\n","\n","{\n","\n","    \"input_word_ids\": ...,\n","\n","    \"input_mask\": ...,\n","\n","    \"input_type_ids\": ...\n","}\n","\n","#input_word_ids\n","\n","→ What it is:\n","A sequence of token IDs (integers) that represent your text after tokenization.\n","\n","→ How it’s made:\n","\n","Your sentence is split into subwords using the WordPiece tokenizer.\n","\n","Each subword (like \"play\", \"##ing\") is mapped to an integer ID from the BERT vocabulary.\n","\n","Special tokens like [CLS] and [SEP] are added.\n","\n","#Example\n","text = \"Playing football\"\n","\n","tokens = [\"[CLS]\", \"play\", \"##ing\", \"football\", \"[SEP]\"]\n","\n","input_word_ids = [101, 2655, 2075, 2374, 102]\n","\n","# input_mask\n","\n","→ What it is:\n","A binary mask indicating which tokens are real words and which are padding.\n","\n","→ Why it’s needed:\n","When BERT processes sentences of varying lengths, shorter ones are padded to a fixed length (say 128 tokens).\n","We don’t want the model to “attend” to padding tokens — so we use a mask.\n","\n","Text: \"Playing football\"\n","Tokenized length: 5\n","Padded length: 8\n","\n","input_mask = [1, 1, 1, 1, 1, 0, 0, 0]\n","\n","\n"," # input_type_ids\n","\n","→ What it is:\n","Indicates which sentence a token belongs to.\n","This is useful when BERT is trained on sentence pairs, such as question-answer or next sentence prediction tasks.\n","\n","[\"This is nice.\", \"Yes it is.\"]\n","\n","[CLS] This is nice [SEP] Yes it is [SEP]\n","\n","input_type_ids = [0,0,0,0,0, 1,1,1,1, 1]\n","\n","\n","\n","Bert Preprocessor does not perform lemmatization or stemming.\n","Those are linguistic preprocessing steps used in classical NLP, not in transformer-based models.\n","BERT learns word meaning directly from large text data, so it doesn’t need lemmatization.\n","\n","#What the model does\n","\n","The model (second hub.KerasLayer) is the actual BERT neural network.\n","It takes the preprocessed tensors from the preprocessor and generates embeddings or contextual representations.\n","\n","\n","\n"],"metadata":{"id":"faV1MBoxUMca"}},{"cell_type":"code","source":["text = [\"I love playing cricket\",\"Cricket is a great game\",\"It is very popular worldwide\"]"],"metadata":{"id":"jpuhh7kEkOuW","executionInfo":{"status":"ok","timestamp":1761155083564,"user_tz":-360,"elapsed":5,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["preprocessed_text = preprocessor(text)\n","preprocessed_text.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CX49RhHymE7M","executionInfo":{"status":"ok","timestamp":1761155086864,"user_tz":-360,"elapsed":260,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"c634475e-0b43-4079-f089-d0eeb5c38dea"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_type_ids', 'input_mask', 'input_word_ids'])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["preprocessed_text['input_mask']"],"metadata":{"id":"Hek4jZWCmQmI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preprocessed_text['input_word_ids']"],"metadata":{"id":"6XHY0mtDm2GG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_result = model(preprocessed_text)\n","bert_result.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gDwPEeimnGkU","executionInfo":{"status":"ok","timestamp":1761155146092,"user_tz":-360,"elapsed":1245,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"2e5d601a-e9d5-42e3-acb6-3d2050440b0f"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['pooled_output', 'sequence_output', 'encoder_outputs', 'default'])"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["#Pooled_output\n","A single vector (embedding) that represents the entire sentence.\n","\n","→ What it’s used for:\n","This is mainly used for sentence-level tasks, such as:\n","\n","Sentiment classification\n","\n","Topic detection\n","\n","Next sentence prediction\n","\n","Basically, if you want to classify the whole input text, use pooled_output."],"metadata":{"id":"kqoO7KV6CZmT"}},{"cell_type":"code","source":["bert_result['pooled_output'][0].shape  #for the first sentence\n","\n","\n","#[batch size, 768] 768 is size of embedding vector"],"metadata":{"id":"jqWSKotKoG3M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761155525035,"user_tz":-360,"elapsed":20,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"daf7a6d9-dc33-4d7a-e53e-3a8dcde1264e"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([768])"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["#Sequence_output\n","\n","→ What it is:\n","\n","This is the hidden representation of every token in the input sequence after all the transformer layers.\n","\n","Shape  [batch_size, sequence_length, hidden_size]\n","\n","[1,128,768]\n","\n","1==> Batch size\n","\n","128 ==> Maximum number of tokens per sentence (sequence length). The 128 includes the [CLS] and [SEP] tokens.\n","\n","768 ==> Embedding dimension for each token\n","\n","#values can be changed according to the verison of Bert"],"metadata":{"id":"JFOfg-xQGcI3"}},{"cell_type":"code","source":["bert_result['sequence_output'] .shape,\n","\n","# [batch_size, seq_length, 768]. 768 vector is for each word in the sentence"],"metadata":{"id":"9_nweF8spgm2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761155850010,"user_tz":-360,"elapsed":23,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"d4eaf6ac-907d-4c81-8135-353473eb5f51"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([3, 128, 768]), TensorShape([3, 768]))"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["len(bert_result['encoder_outputs']) # 6 means number of encoders"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PxwE_tS2qtjU","executionInfo":{"status":"ok","timestamp":1760075442372,"user_tz":-360,"elapsed":15,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"b96b4d92-fc80-4480-f963-75865a0bc3f6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["bert_result['encoder_outputs'][0]\n","\n","#Each item is the sequence_output from a specific layer — helpful if you want to do layer-wise feature extraction or visualize attention."],"metadata":{"id":"Xa1UvzTCrZxW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Implementation from Hugging face Transformer"],"metadata":{"id":"3QXOx5e0M4mS"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"lkJE7EyuNc-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel\n","import torch\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertModel.from_pretrained(\"bert-base-uncased\")\n","\n","text = \"ChatGPT is a language model developed by OpenAI.\"\n","inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n","outputs = model(**inputs)\n","\n","print(outputs.keys())  # odict_keys(['last_hidden_state', 'pooler_output'])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EMInNvhxPY7D","executionInfo":{"status":"ok","timestamp":1761159805058,"user_tz":-360,"elapsed":4940,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"aba9a458-90a2-4e79-98b1-ae3b33262c8b"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["odict_keys(['last_hidden_state', 'pooler_output'])\n"]}]},{"cell_type":"code","source":["outputs['pooler_output'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZ7xsYGTTh3q","executionInfo":{"status":"ok","timestamp":1761159857000,"user_tz":-360,"elapsed":23,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"12261478-4a75-486e-b9bd-e2c0cbded72f"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 768])"]},"metadata":{},"execution_count":28}]}]}