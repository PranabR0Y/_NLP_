{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/M5GFps2teBf9gJfS8MnG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#NLTK (Natural Language Toolkit)"],"metadata":{"id":"002rmd5aajcB"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9QGN7qUdZ1Gv","executionInfo":{"status":"ok","timestamp":1760242579794,"user_tz":-360,"elapsed":8527,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"d8d4d94e-56d7-4388-94ed-776bbbf7a68b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"]}],"source":["! pip install nltk"]},{"cell_type":"code","source":["import nltk\n","nltk.download('all')# for other  languages"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"niTxTiP1bPIM","executionInfo":{"status":"ok","timestamp":1760242739892,"user_tz":-360,"elapsed":56361,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"ecadc7d1-b18f-4072-e71a-bf30bc733c79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package english_wordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n","[nltk_data]    | Downloading package mock_corpus to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mock_corpus.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package tagsets_json to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets_json.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["#Tokenization\n","Tokenization refers to break down the text into smaller units. It splits paragraphs into sentences and sentences into words. It is one of the initial steps of any NLP pipeline. Let us have a look at the two major kinds of tokenization that NLTK provides:"],"metadata":{"id":"zpu4w_7Yb5sq"}},{"cell_type":"code","source":["from nltk import word_tokenize, sent_tokenize\n","\n","sentence = \"Natural Language Processing is a very popular topic in Machine Learning. It is very interesting\"\n","\n","print(word_tokenize(sentence))\n","print(sent_tokenize(sentence))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"flmsOey-bbek","executionInfo":{"status":"ok","timestamp":1760242759587,"user_tz":-360,"elapsed":90,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"dcbb85ea-020b-4686-de7e-85ad675908b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Natural', 'Language', 'Processing', 'is', 'a', 'very', 'popular', 'topic', 'in', 'Machine', 'Learning', '.', 'It', 'is', 'very', 'interesting']\n","['Natural Language Processing is a very popular topic in Machine Learning.', 'It is very interesting']\n"]}]},{"cell_type":"markdown","source":["# Stemming\n","Stemming generates the base word from the given word by removing the affixes of the word."],"metadata":{"id":"DHZ3IWdudlrV"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","stemmer = PorterStemmer()\n","\n","print(stemmer.stem(\"playing\"))\n","print(stemmer.stem(\"plays\"))\n","print(stemmer.stem(\"played\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N80dkS3adn6n","executionInfo":{"status":"ok","timestamp":1760186134567,"user_tz":-360,"elapsed":16,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"7c81eab8-7fb7-46c4-f60e-6e31021342f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["play\n","play\n","play\n"]}]},{"cell_type":"markdown","source":[" # Lemmatization\n","Lemmatization involves grouping together the inflected forms of the same word. Unlike stemming which simply removes prefixes or suffixes, it considers the word's meaning and part of speech (POS) and ensures that the base form is a valid word. This makes lemmatization more accurate as it avoids generating non-dictionary words."],"metadata":{"id":"30q0-n_qeTgj"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","print(lemmatizer.lemmatize(\"playing\",'v'))\n","print(lemmatizer.lemmatize(\"plays\",'v'))\n","print(lemmatizer.lemmatize(\"played\",'v'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3tXXbXAAecq6","executionInfo":{"status":"ok","timestamp":1760186139543,"user_tz":-360,"elapsed":3156,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"5a009058-7c35-4b1a-a784-17f4cac2f98f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["play\n","play\n","play\n"]}]},{"cell_type":"markdown","source":["# Part of Speech Tagging\n","Part of Speech (POS) tagging refers to assigning each word of a sentence to its part of speech. It is significant as it helps to give a better syntactic overview of a sentence."],"metadata":{"id":"JIiEGpX4fKWX"}},{"cell_type":"code","source":["from nltk import pos_tag\n","from nltk import word_tokenize\n","\n","text = \"GeeksforGeeks is best platform for Computer Science Students\"\n","tokens = word_tokenize(text)\n","print(pos_tag(tokens))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rIuEXdp2fJWj","executionInfo":{"status":"ok","timestamp":1760200200501,"user_tz":-360,"elapsed":337,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"c2354fa6-3b26-47f2-b5ed-d31db65acc70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('GeeksforGeeks', 'NNP'), ('is', 'VBZ'), ('best', 'JJS'), ('platform', 'NN'), ('for', 'IN'), ('Computer', 'NNP'), ('Science', 'NNP'), ('Students', 'NNS')]\n"]}]},{"cell_type":"markdown","source":["#Tokenization Using Spacy\n","we are using SpaCy's blank model (spacy.blank(\"en\")) which initializes a minimal pipeline without pre-trained components like part-of-speech tagging or named entity recognition."],"metadata":{"id":"sKU18yyURah4"}},{"cell_type":"code","source":["import spacy\n","\n","nlp = spacy.blank(\"en\")\n","\n","doc = nlp(\"Natural Language Processing is a very popular topic in Machine Learning. It is very interesting\")\n","\n","\n","for token in doc:\n","  print(token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A616qWpgRbx4","executionInfo":{"status":"ok","timestamp":1760271224562,"user_tz":-360,"elapsed":12497,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"47525ad7-7132-4627-b1c8-0d0ba84a9842"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Natural\n","Language\n","Processing\n","is\n","a\n","very\n","popular\n","topic\n","in\n","Machine\n","Learning\n",".\n","It\n","is\n","very\n","interesting\n"]}]},{"cell_type":"code","source":["dir(doc[0])"],"metadata":{"id":"YCYbJ1Djpm7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text='''\n","Look for data to help you address the question. Governments are good\n","sources because data from public research is often freely available. Good\n","places to start include http://www.data.gov/, and http://www.science.\n","gov/, and in the United Kingdom, http://data.gov.uk/.\n","Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/,\n","and the European Social Survey at http://www.europeansocialsurvey.org/.\n","'''\n","\n","doc = nlp(text)\n","\n","for token in doc:\n","  if token.like_url:\n","    print(token.text)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pf6rykdmte4g","executionInfo":{"status":"ok","timestamp":1760244415842,"user_tz":-360,"elapsed":60,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"cb763854-f59e-4e53-e48a-159b4a78d39a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["http://www.data.gov/\n","http://www.science\n","http://data.gov.uk/.\n","http://www3.norc.org/gss+website/\n","http://www.europeansocialsurvey.org/.\n"]}]},{"cell_type":"code","source":["transactions = \"Tony gave two $ to Peter, Bruce gave 500 ‚Ç¨ to Steve\"\n","doc = nlp(transactions)\n","\n","for i in range(len(doc)-1):\n","  if doc[i].like_num and doc[i+1].is_currency:\n","    print(doc[i].text,' ',doc[i+1].text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-1I2qC-OvliO","executionInfo":{"status":"ok","timestamp":1760245506385,"user_tz":-360,"elapsed":46,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"95c3c5db-80d3-4018-ead7-b9f64da93e19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["two   $\n","500   ‚Ç¨\n"]}]},{"cell_type":"markdown","source":["#Displaying the Pipeline Components\n","We use the pre-trained en_core_web_sm model which includes various components for NLP tasks. After loading the model, we can display the available components in the pipeline"],"metadata":{"id":"beDT3HriSa9S"}},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_sm\")\n","nlp.pipe_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eubUZEVtSgft","executionInfo":{"status":"ok","timestamp":1760271240904,"user_tz":-360,"elapsed":1307,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"0697a8a5-873d-4d39-aee4-e2b38f1f0c77"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["| Pipeline Step       | Purpose                              | Example                       |\n","| ------------------- | ------------------------------------ | ----------------------------- |\n","| **tok2vec**         | Converts tokens into numeric vectors | ‚ÄúApple‚Äù ‚Üí [0.1, -0.2, 0.5‚Ä¶]   |\n","| **tagger**          | Assigns part-of-speech tags          | ‚Äúbuying‚Äù ‚Üí VERB               |\n","| **parser**          | Finds dependency relations           | ‚ÄúApple‚Äù ‚Üí subject of ‚Äúbuying‚Äù |\n","| **attribute_ruler** | Adjusts linguistic attributes        | Fixes contractions, rules     |\n","| **lemmatizer**      | Finds base form of words             | ‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù             |\n","| **ner**             | Detects named entities               | ‚ÄúApple‚Äù ‚Üí ORG                 |\n","\n","| Model            | Vector Source                       | Dimension  | Type               |\n","| ---------------- | ----------------------------------- | ---------- | ------------------ |\n","| `en_core_web_sm` | ‚ö†Ô∏è **No pre-trained word vectors**  | 0D (empty) | Small, lightweight |\n","| `en_core_web_md` | Medium-sized pre-trained embeddings | 300D       | GloVe-style        |\n","| `en_core_web_lg` | Large pre-trained embeddings        | 300D       | Better quality     |\n"],"metadata":{"id":"ZKBuZnHpHM5x"}},{"cell_type":"code","source":["print(doc[0].text, doc[0].vector[:5])\n","\n","print(doc[0].vector.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yc1S_YL8Iy9c","executionInfo":{"status":"ok","timestamp":1760201119946,"user_tz":-360,"elapsed":33,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"34c3cf3f-35f1-4b83-d009-5763586bb112"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Natural []\n","(0,)\n"]}]},{"cell_type":"code","source":["!python -m spacy download en_core_web_md\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1JmSopxkK7dE","executionInfo":{"status":"ok","timestamp":1760201275895,"user_tz":-360,"elapsed":14267,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"8f13f81b-d95d-49b6-d543-9e86e63d547c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-md==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: en-core-web-md\n","Successfully installed en-core-web-md-3.8.0\n","\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_md')\n","\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_md\")\n","\n","doc = nlp(\"Natural Language Processing is a very popular topic in Machine Learning. It is very interesting\")\n","\n","\n","print(doc[0].text,doc[0].vector[:5],doc[0].vector.shape)\n","\n","print(doc[15].similarity(doc[6]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hOEohpsdKcsF","executionInfo":{"status":"ok","timestamp":1760201618798,"user_tz":-360,"elapsed":1871,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"1ec483ca-150c-4c71-cdfd-ca59bd9fe490"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Natural [-0.66059   0.2348   -0.021227 -0.32737  -0.062493] (300,)\n","0.497653067111969\n"]}]},{"cell_type":"code","source":["for token in doc:\n","  print(token.text,\"   \", token.tag_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PaBnqBD_NlRV","executionInfo":{"status":"ok","timestamp":1760202033110,"user_tz":-360,"elapsed":24,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"00aa14b1-0f7b-4ada-aee2-67b3e0b72e8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Natural     NNP\n","Language     NNP\n","Processing     NNP\n","is     VBZ\n","a     DT\n","very     RB\n","popular     JJ\n","topic     NN\n","in     IN\n","Machine     NNP\n","Learning     NNP\n",".     .\n","It     PRP\n","is     VBZ\n","very     RB\n","interesting     JJ\n"]}]},{"cell_type":"code","source":["for token in doc:\n","    print(f\"{token.text:<10} {token.dep_:<10} ‚Üí {token.head.text}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22XDu9CmQEr3","executionInfo":{"status":"ok","timestamp":1760202611308,"user_tz":-360,"elapsed":58,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"74cebb5c-1c94-43fa-88e8-81af0ddfeaa7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Natural    compound   ‚Üí Language\n","Language   compound   ‚Üí Processing\n","Processing nsubj      ‚Üí is\n","is         ROOT       ‚Üí is\n","a          det        ‚Üí topic\n","very       advmod     ‚Üí popular\n","popular    amod       ‚Üí topic\n","topic      attr       ‚Üí is\n","in         prep       ‚Üí topic\n","Machine    compound   ‚Üí Learning\n","Learning   pobj       ‚Üí in\n",".          punct      ‚Üí is\n","It         nsubj      ‚Üí is\n","is         ROOT       ‚Üí is\n","very       advmod     ‚Üí interesting\n","interesting acomp      ‚Üí is\n"]}]},{"cell_type":"code","source":["for token in doc:\n","    print(token.text,\" \" ,token.lemma_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"60ZbzTSTQJwB","executionInfo":{"status":"ok","timestamp":1760202689659,"user_tz":-360,"elapsed":40,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"1e12ef97-41e8-47a0-97f9-4070006006cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Natural   Natural\n","Language   Language\n","Processing   Processing\n","is   be\n","a   a\n","very   very\n","popular   popular\n","topic   topic\n","in   in\n","Machine   Machine\n","Learning   Learning\n",".   .\n","It   it\n","is   be\n","very   very\n","interesting   interesting\n"]}]},{"cell_type":"code","source":["doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n","for ent in doc.ents:\n","    print(ent.text, ent.label_,\"->\",spacy.explain(ent.label_))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iuIoVgY1QhbK","executionInfo":{"status":"ok","timestamp":1760271380054,"user_tz":-360,"elapsed":24,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"a64d9c5f-36c1-4e51-bb89-eacd4ca39e6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tesla Inc ORG -> Companies, agencies, institutions, etc.\n","$45 billion MONEY -> Monetary values, including unit\n"]}]},{"cell_type":"markdown","source":["#TextBlob\n","TextBlob is a simple, beginner-friendly NLP library built on top of NLTK and Pattern.\n","It provides an easy API for common text-processing tasks ‚Äî without needing to deal with complex NLP pipelines.\n","\n","| Purpose                                 | Description                                   | Example                                     |\n","| --------------------------------------- | --------------------------------------------- | ------------------------------------------- |\n","| üó£Ô∏è **Sentiment Analysis**              | Detect positive/negative/neutral tone in text | ‚ÄúI love this phone‚Äù ‚Üí `polarity = 0.5`      |\n","| üß© **Tokenization**                     | Split text into words/sentences               | `\"I like NLP.\" ‚Üí ['I', 'like', 'NLP', '.']` |\n","| üè∑Ô∏è **POS Tagging**                     | Identify parts of speech (noun, verb, etc.)   | `\"running\" ‚Üí VERB`                          |\n","| üß† **Noun Phrase Extraction**           | Extract important phrases                     | `\"the smart student\"`                       |\n","| üî§ **Lemmatization / Word Inflection**  | Convert between word forms                    | `\"better\" ‚Üí \"good\"`                         |\n","| üåç **Translation & Language Detection** | Translate text (using online API)             | English ‚Üí Spanish                           |\n"],"metadata":{"id":"XnanEf3bSqu4"}},{"cell_type":"code","source":["\n","from textblob import TextBlob\n","\n","blob = TextBlob(\"TextBlob makes NLP simple. It is also fun to use!\")\n","\n","blob2 = TextBlob(\"I hate bugs in my code.\")\n","\n","\n","#Sentiment Analysis\n","print(blob.sentiment,blob2.sentiment)\n","\n","#Polarity: from -1 (negative) to 1 (positive)\n","\n","#Subjectivity: from 0 (objective) to 1 (subjective)\n","\n","#Tokenization\n","\n","print(blob.words,blob.sentences)\n","\n","#Noun Phrase Extraction\n","print(blob.noun_phrases)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VwHdT0exS8Hd","executionInfo":{"status":"ok","timestamp":1760204634337,"user_tz":-360,"elapsed":37,"user":{"displayName":"Pranab Roy","userId":"03404794500260995570"}},"outputId":"b16ce9ab-eb05-4a85-fae2-bcebd7f80a70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentiment(polarity=0.1875, subjectivity=0.2785714285714286) Sentiment(polarity=-0.8, subjectivity=0.9)\n","['TextBlob', 'makes', 'NLP', 'simple', 'It', 'is', 'also', 'fun', 'to', 'use'] [Sentence(\"TextBlob makes NLP simple.\"), Sentence(\"It is also fun to use!\")]\n","['textblob', 'nlp']\n"]}]}]}